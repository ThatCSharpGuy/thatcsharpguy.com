<!DOCTYPE html>
<html lang=""  itemscope itemtype="http://schema.org/Blog">
<head>
        <!–– Made with Lockdown: https://github.com/fferegrino/lockdown ––>
<meta charset="utf-8" />
<link rel="stylesheet" href="/css/main.css" />

        
</head>

<body>  
        <header class="site-header ui">
    <div class="wrapper menu">
      <nav class="site-nav">
        <a href="#" class="menu-icon">
          <svg viewBox="0 0 18 15">
            <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
            <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
            <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
          </svg>
        </a>
        <div class="trigger">
          <a class="link page-link" href="/">Home</a>
          <a class="link page-link" href="/about">About</a>
          <a class="link page-link" href="/search">Search</a>
        </div>
      </nav>
    </div>
    <div class="wrapper title">
      <div class="pure-g">
        <div class="pure-u-1-24 title-number"><span>1</span></div>
        <div class="pure-u-11-24 content-window">
          <a class="site-title" href="/">That C# guy</a>
        </div>
        <div class="pure-u-12-24 header-menu">
          <ul>
            <li class="datascience"><a href="/tag/data-science/">Data Science!</a></li>
            <li class="youtube"><a href="https://www.youtube.com/c/thatcsharpguy">YouTube<a></li>
            <li class="csharp"><a href="/tag/aprendecsharp/">Aprende C#</a></li>
            <li class="xamarin"><a href="/tag/xamarin/">Xamarin</a></li>
          </ul>
        </div>
      </div>
  
  </header>
        <div class="page-content">
            <div class="wrapper">
                
<h1></h1>

<h1 id="text-as-data">Text as Data</h1>
<p>Text as data... a kind of a generic name to talk about text analysis &amp; text classification.</p>
<p>The idea behind this course was to teach us the basics of three things taking into consideration that in the real world, information is in significant part contained in text documents:</p>
<p>The first thing is to learn how to <strong>organise and categorise</strong> text. The second was how to <strong>search and retrieve</strong> the documents or fragments of them, and the third one was how to <strong>analyse</strong> the text to extract the sentiments that the authors were expressing.</p>
<h2 id="lecture-1.introduction-to-text">Lecture 1. Introduction to text</h2>
<p>In the first lecture, we reviewed how can text be represented as <strong>sparse vectors</strong>, how can we calculate different <strong>similarity measures</strong> between two vectors using similarity measures of sets. After that, we checked the <strong>bag of words</strong> representation, as well how can we go beyond working with single, tokenised words and consider pairs or triples of words using <strong>n-grams</strong> and another similarity measure, the <strong>cosine similarity</strong>. We finalised this lecture by reviewing the problems with using term frequency as the only criteria to describe our documents.</p>
<h2 id="lecture-2.text-distributions">Lecture 2. Text distributions</h2>
<p>As we learned in the previous lecture, using the <em>raw term frequency</em> is not the best idea, and thus, in this lecture, we saw different options to overcome this issue, such as <strong>operating in the log space</strong> for the term frequency, as well as how to consider the collection using something known as the <strong>inverse document frequency or IDF</strong>.</p>
<h2 id="lecture-3.distributions-and-clustering">Lecture 3. Distributions and clustering</h2>
<p>Following with term distribution, we saw how it could be used to cluster documents in an unsupervised manner using algorithms such as k-means or hierarchical clustering.</p>
<h2 id="lecture-4.language-modelling">Lecture 4. Language Modelling</h2>
<p>For the fourth lecture, we learned another approach to representing documents and that is through probabilities: the probability of a sequence of words and the probability of a word given a sequence of words, via Language Models, and how considering n-grams allows us to get better models.</p>
<h2 id="lecture-5.word-vectors">Lecture 5. Word Vectors</h2>
<p>In lecture number five we learned about word embeddings, which is a somewhat more modern approach of representing words as dense vectors, created from the context of each word.</p>
<h2 id="lecture-6.text-classification">Lecture 6. Text classification</h2>
<p>Lecture six was about classification; we briefly reviewed classifiers such as Naïve Bayes, logistic regression, SVM and decision trees.</p>
<h2 id="lecture-7.intro-to-nlp">Lecture 7. Intro to NLP</h2>
<p>Natural Language Processing was the topic of the seventh lecture, in this case, things like including part of the speech tagging and dependency parsing.</p>
<h2 id="lecture-8.more-on-text-classification">Lecture 8. More on text classification</h2>
<p>Lecture eight was another look at classification, reviewing some good practices to avoid over or underfitting, as well as some ethical concerns that may arise from using machine learning for real-world applications.</p>
<h2 id="lecture-9.more-on-clustering">Lecture 9. More on clustering</h2>
<p>Just like the previous lecture, this one was about revisiting an old lecture from another perspective, in this case: clustering using <strong>Latent Semantic Indexing</strong>.</p>
<h2 id="applications">Applications</h2>
<p>The last lectures were about applications of what we saw during the course:</p>
<h3 id="lecture-10.information-extraction-named-entity-recognition-and-relation-extraction">Lecture 10. Information Extraction, Named Entity recognition and Relation Extraction</h3>
<h3 id="lecture-11.question-answering-and-qa-architectures">Lecture 11. Question Answering, and QA architectures</h3>
<h3 id="lecture-12.dialogue-systems-chatbots-slot-filling">Lecture 12. Dialogue Systems, chatbots, slot filling</h3>
<h2 id="labs">Labs.</h2>
<p>The labs for this course were by far the most interesting of any other course I had this semester (don't feel bad Big Data, yours were cool as well). We worked with tools like NLTK and spaCy, and Google's version of the Jupyter Notebooks called Colab.</p>


            </div>
          </div>
        
</body>
</html>
